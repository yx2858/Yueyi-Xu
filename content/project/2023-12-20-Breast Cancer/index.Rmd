---
title: "Predicting Mortality Status in Patients with Breast Cancer Based on Multiple Approaches"
author: "Jixin Li, Yueyi Xu, Peng Su, Tianhui Huang"
date: "2023-12-11"
excerpt: For my Biostatistical Method I class, my group used Logistic Regression, Survival Analysis, Decision Tree, and Random Forest to predict mortality status in patients with breat cancer.
---

```{r message=FALSE, warning=FALSE, include = F}
library(tidyverse)
library(performance)
library(MASS)
library(glmnet)
library(dplyr)
library(survival)
library(lmtest)
library(pROC)
library(rms)
library(caTools)
library(e1071)
library(caTools)
library(class)
library(randomForest)
library(tibble)
library(datasets)
library(party)
library(magrittr)
library(rpart)
library(knitr)
```

<!--# logistic Regression-->

<!--## clean data-->
```{r, include = F}
training_df = 
   read_csv("Training.csv") |>
   janitor::clean_names()
```

```{r, include = F}
training_df =
training_df |>
   mutate(             
   status = replace(status, status == "Alive", 1),
   status = replace(status, status == "Dead", 0),
   status = as.numeric(status)) 
```

## Abstract

This study evaluates the predictive performance of four methods—logistic regression, survival analysis, decision tree, and random forest—in determining the status of breast cancer patients. The results indicate that random forest exhibits the highest prediction accuracy, reaching 89.59%. Additionally, although variations in accuracy exist among the models, there are minimal differences in accuracy within each model when predicting outcomes for both white and black individuals.

## Introduction

Breast cancer is the most common cancer in women and the second leading cause of cancer death in women in the World [1]. In recent years, the incidence of breast cancer has been on the rise, and approximately 685,000 women in the world die from breast cancer every year [2]. Therefore, it is crucial to effectively predict the death status of breast cancer patients. This study uses four methods to predict the death status of breast cancer patients. The second section outlines the steps and algorithms of each method. The third section presents the fitted model and its accuracy, and the fourth section compares and summarizes the results.

## Methods
### Logistics Regression

The first method is intended to conduct a series of logistic regression analyses and explore the interaction between different variables. The process begins by loading the training dataset and converting the status variable to binary numbers. After diagnosing assumptions, variables demonstrating multicollinearity are omitted, and two outliers are removed, a new dataset called “trainOut” is created. Then the logistic regression is fitted (“new_model”) and the first round of feature selection is performed to select the most relevant variables. After that, the most significant interaction is evaluated and added to the model, called “best_fit”. Finally, the second round feature selection is performed on the model with interaction effect and the final model is obtained, called “best_fit2”. The accuracy of best_fit2 is then predicted by applying it to the testing dataset. Lastly, separate accuracy predictions are then carried out for each of these racial categories (white / black) to evaluate potential variations in model performance.

<!-- ## fit a logistic regression model -->

```{r, include = F}
model_df = glm(status ~ ., data = training_df)

summary(model_df)
```

<!-- ## check for assumptions for logistic regression model -->
```{r, include = F}
check_collinearity(model_df)
plot(model_df, which = 4)
```

```{r, include = F}
trainOut = training_df[-c(1004,2114),]
```

<!--## fit a new logistic regression model--> 
```{r, include = F}
new_model = glm(status ~ age + race + marital_status + t_stage + differentiate + grade + a_stage + tumor_size + estrogen_status + progesterone_status + regional_node_examined + reginol_node_positive + survival_months, data = trainOut)

summary(new_model)
```

```{r, include = F}
check_collinearity(new_model)
```

<!--## interaction -->
```{r, include = F}
model1 = glm(status ~ age + race + age*race, data = trainOut)
summary(model1)

model2 = glm(status ~ age + t_stage + age*t_stage, data = trainOut)
summary(model2)

model3 = glm(status ~ age + differentiate + age*differentiate, data = trainOut)
summary(model3)

model4 = glm(status ~ age + estrogen_status + age*estrogen_status, data = trainOut)
summary(model4)

model5 = glm(status ~ age +  progesterone_status + age* progesterone_status, data = trainOut)
summary(model5)

model6 = glm(status ~ age^2 + log(regional_node_examined) + age^2*log(regional_node_examined), data = trainOut)
summary(model6)

model7 = glm(status ~ age^2 + log(reginol_node_positive) + age^2*log(reginol_node_positive), data = trainOut)
summary(model7)

## significant 
model8 = glm(status ~ age^2 + survival_months^2 + age^2*survival_months^2, data = trainOut)
summary(model8)

model9 = glm(status ~ race + t_stage + race*t_stage, data = trainOut)
summary(model9)

model10 = glm(status ~ race + differentiate + race*differentiate, data = trainOut)
summary(model10)

model11 = glm(status ~ race + estrogen_status + race*estrogen_status, data = trainOut)
summary(model11)

model12 = glm(status ~ race + progesterone_status + race*progesterone_status, data = trainOut)
summary(model12)

model13 = glm(status ~ race + log(regional_node_examined) + race*log(regional_node_examined), data = trainOut)
summary(model13)

model14 = glm(status ~ race + log(reginol_node_positive) + race*log(reginol_node_positive), data = trainOut)
summary(model14)

## significant 
model15 = glm(status ~ race + survival_months^2 + race*survival_months^2, data = trainOut)
summary(model15)

model16 = glm(status ~ t_stage + differentiate + t_stage*differentiate, data = trainOut)
summary(model16)

## significant 
model17 = glm(status ~ t_stage + estrogen_status + t_stage*estrogen_status, data = trainOut)
summary(model17)

## significant
model18 = glm(status ~ t_stage + progesterone_status + t_stage*progesterone_status, data = trainOut)
summary(model18)

model19 = glm(status ~ t_stage + log(regional_node_examined) + t_stage*log(regional_node_examined), data = trainOut)
summary(model19)

## slight significant 
model20 = glm(status ~ t_stage + log(reginol_node_positive) + t_stage*log(reginol_node_positive), data = trainOut)
summary(model20)

## significant
model21 = glm(status ~ t_stage + survival_months^2 + t_stage*survival_months^2, data = trainOut)
summary(model21)

model22 = glm(status ~ differentiate + estrogen_status + differentiate*estrogen_status, data = trainOut)
summary(model22)

## significant
model23 = glm(status ~ differentiate + progesterone_status + differentiate*progesterone_status, data = trainOut)
summary(model23)

model24 = glm(status ~ differentiate + log(regional_node_examined) + differentiate*log(regional_node_examined), data = trainOut)
summary(model24)

model25 = glm(status ~ differentiate + log(reginol_node_positive) + differentiate*log(reginol_node_positive), data = trainOut)
summary(model25)

## significant
model26 = glm(status ~ differentiate + survival_months^2 + differentiate*survival_months^2, data = trainOut)
summary(model26)

model27 = glm(status ~ estrogen_status + progesterone_status + estrogen_status*progesterone_status, data = trainOut)
summary(model27)

model28 = glm(status ~ estrogen_status + log(regional_node_examined) + estrogen_status*log(regional_node_examined), data = trainOut)
summary(model28)

model29 = glm(status ~ estrogen_status + log(reginol_node_positive) + estrogen_status*log(reginol_node_positive), data = trainOut)
summary(model29)

## significant
model30 = glm(status ~ estrogen_status + survival_months^2 + estrogen_status*survival_months^2, data = trainOut)
summary(model30)

model31 = glm(status ~ progesterone_status + log(regional_node_examined) + progesterone_status*log(regional_node_examined), data = trainOut)
summary(model31)

## slight significant
model32 = glm(status ~ progesterone_status + log(reginol_node_positive) + progesterone_status*log(reginol_node_positive), data = trainOut)
summary(model32)

## significant
model33 = glm(status ~ progesterone_status + survival_months^2 + progesterone_status*survival_months^2, data = trainOut)
summary(model33)

model34 = glm(status ~ log(regional_node_examined) + log(reginol_node_positive) + log(regional_node_examined)*log(reginol_node_positive), data = trainOut)
summary(model34)

model35 = glm(status ~ log(regional_node_examined) + survival_months^2 + log(regional_node_examined)*survival_months^2, data = trainOut)
summary(model35)

## significant
model36 = glm(status ~ log(reginol_node_positive) + survival_months^2 + log(reginol_node_positive)*survival_months^2, data = trainOut)
summary(model36)
```

<!--## significant interaction -->
```{r, include = F}
model8 = glm(status ~ age + survival_months + age*survival_months, data = trainOut)
summary(model8)

model15 = glm(status ~ race:survival_months, data = trainOut)
summary(model15)

model17 = glm(status ~ t_stage + estrogen_status + t_stage*estrogen_status, data = trainOut)
summary(model17)

model18 = glm(status ~ t_stage + progesterone_status + t_stage*progesterone_status, data = trainOut)
summary(model18)

model20 = glm(status ~ t_stage + reginol_node_positive + t_stage*reginol_node_positive, data = trainOut)
summary(model20)

model21 = glm(status ~ t_stage + survival_months + t_stage*survival_months, data = trainOut)
summary(model21)

model23 = glm(status ~ differentiate + progesterone_status + differentiate*progesterone_status, data = trainOut)
summary(model23)

model26 = glm(status ~ differentiate + survival_months + differentiate*survival_months, data = trainOut)
summary(model26)

model30 = glm(status ~ estrogen_status + survival_months + estrogen_status*survival_months, data = trainOut)
summary(model30)


model33 = glm(status ~ progesterone_status + survival_months + progesterone_status*survival_months, data = trainOut)
summary(model33)

model36 = glm(status ~ reginol_node_positive + survival_months + reginol_node_positive*survival_months, data = trainOut)
summary(model36)
```

<!--## model selection--> 
```{r, include = F}
step(new_model, direction='both')
```

```{r, include = F}
best_fit = glm(formula = status ~ age + race + t_stage + differentiate + 
    estrogen_status + progesterone_status + regional_node_examined + 
    reginol_node_positive + survival_months+
      t_stage:regional_node_examined+t_stage:estrogen_status+race:survival_months+
      t_stage:survival_months+differentiate:progesterone_status+differentiate:survival_months+
      estrogen_status:survival_months+progesterone_status:survival_months+
      reginol_node_positive:survival_months, data = trainOut)

summary(best_fit)
```

```{r, include = F}
step(best_fit, direction='both')


best_fit2 = glm(formula = status ~ age + race + t_stage + differentiate + 
    estrogen_status + progesterone_status + regional_node_examined + 
    reginol_node_positive + survival_months + race:survival_months + 
    t_stage:survival_months + differentiate:progesterone_status + 
    differentiate:survival_months + progesterone_status:survival_months + 
    reginol_node_positive:survival_months, data = trainOut)

summary = best_fit2$coefficients

summary(best_fit2)
```

```{r, echo = F}
summary |>
   knitr::kable()
```


```{r, include = F}
test_data<-read.csv("testing.csv") |>
  janitor::clean_names() |>
   mutate(             
   status = replace(status, status == "Alive", 1),
   status = replace(status, status == "Dead", 0),
   status = as.numeric(status)) 
```

<!--## prediction -->
```{r, include = F}
test_prediction<-predict(best_fit2,test_data)
test_prediction_binary<-ifelse(test_prediction>0.5,1,0)
prediction_power<-mean(test_prediction_binary==test_data$status)
print(prediction_power)


test_white<-subset(test_data,race=="White")
test_black<-subset(test_data,race=="Black")

## White
test_prediction_white<-predict(best_fit,test_white)
test_prediction_binary_white<-ifelse(test_prediction_white>0.5,1,0)
prediction_power_white<-mean(test_prediction_binary_white==test_white$status)
print(prediction_power_white) ## 88.05687%


## Balck
test_prediction_black<-predict(best_fit,test_black)
test_prediction_binary_black<-ifelse(test_prediction_black>0.5,1,0)
prediction_power_black<-mean(test_prediction_binary_black==test_black$status)
print(prediction_power_black) ## 85%
```


## Method
### Survival Analysis

To find the effect of covariates on survival time and their contributions to the outcome, survival analysis using the Cox proportional hazards model is conducted, which allows to compare hazard ratios among the covariates.
1.	Fitted a full Cox model with all variables and possible pairwise interactions as predictors on training data
2.	After evaluating model fit and testing assumptions by “cox.zph()” function, variables with non-significant p-values, which meet the assumptions, were selected, and a new Cox model
3.	The variable selection process was performed by stepwise selection and the final model was tested by the Receiver Operating Characteristic Curve (ROC) and the model’s Area Under the Curve (AUC)
4.	The difference in prediction accuracy of the race was adjusted by adding “weight” parameter to the model.

<!--# survival analysis -->

```{r, include = F}
#import data
Training <- read_csv("Training.csv")
Testing <- read_csv("Testing.csv")

Training = 
  Training |>
  janitor::clean_names() |>
  mutate(
    status = case_match(
      status,
       "Alive" ~ 0,
       "Dead" ~ 1
    )
  )

Testing = 
  Testing |>
  janitor::clean_names() |>
  mutate(
    status = case_match(
      status,
       "Alive" ~ 0,
       "Dead" ~ 1    )
  )
```


```{r message=FALSE, warning=FALSE, include = F}
#model building and assumption test
set.seed(97)
cox_model <- coxph(Surv(survival_months, status) ~ . + t_stage*n_stage + 
                     estrogen_status*progesterone_status + age*grade +
                     tumor_size*regional_node_examined, data = Training)

cox_zph_result <- cox.zph(cox_model)
cox_zph_result
```

```{r, echo = F}
par(mfrow = c(2,2))
plot(cox_zph_result[8], ylab = "residuals of a_stage")
plot(cox_zph_result[10], ylab = "residuals of estrogen_status")
plot(cox_zph_result[11], ylab = "residuals of progesterone_status")
plot(cox_zph_result[15], ylab = " estrogen_status:progesterone_status")
```

```{r, include = F}
cox_model_ass <- coxph(Surv(survival_months, status) ~ . - a_stage - estrogen_status - progesterone_status + t_stage*n_stage + 
                     tumor_size*regional_node_examined, data = Training)

cox_zph_result <- cox.zph(cox_model_ass)
cox_zph_result

#model selection
step(cox_model_ass, direction =  "backward")
cox_model_final <- coxph(Surv(survival_months, status) ~ age + race + 
    marital_status + t_stage + n_stage + differentiate + regional_node_examined + 
    reginol_node_positive, data = Training)
```
  
```{r, echo = F}
as.data.frame(coef(summary(cox_model_final))) |>
  arrange(`Pr(>|z|)`) |>
  knitr::kable(col.names = c("term","coef", "exp(coef)" , "se(coef)","z", "Pr(>z)"),digits = 4)
```

```{r, echo = F}
#accuracy of the final model
predicted_classes <- ifelse(predict(cox_model_final,type='lp',newdata=Testing) > 0.5, 1, 0)
accuracy <- mean(predicted_classes == Testing$status)
cat("Accuracy", accuracy, "\n")
```


```{r, include = F}
#for race accuracy
#all race
fold_test <- Testing
fold_train <- Training

fold_pre <- cox_model_final
fold_predict <- predict(fold_pre,type='lp',newdata=fold_test)
roc1<-roc(fold_test$status,fold_predict)

#white
par(mfrow = c(1,1))
fold_test_w <- Testing |> filter(race == "White")
fold_train_w <- Training |> filter(race == "White")

fold_pre_w <- cox_model_final 
fold_predict_w <- predict(fold_pre_w,type='lp',newdata=fold_test_w)
roc2<-roc(fold_test_w$status,fold_predict_w)

#black
fold_test_b <- Testing |> filter(race != "White")
fold_train_b <- Training |> filter(race != "White")

fold_pre_b <- cox_model_final 
fold_predict_b <- predict(fold_pre_b,type='lp',newdata=fold_test_b)
roc3<-roc(fold_test_b$status,fold_predict_b)
```

```{r, echo = F}
#ROC plot
plot(roc1, col = "blue", main = "Receiver Operating Characteristic (ROC) Curve", 
     lwd = 2, lty = 1,print.auc=T)

lines(roc2, col = "red", lwd = 2, lty = 2)
lines(roc3, col = "orange", lwd = 2, lty = 3)
legend("bottomright", legend = c("Main model", "White", "Black and Others"),
       col = c("blue", "red", "orange"), lty = c(1, 2, 3), lwd = 2)
```

```{r, include = F}
#accuracy for white
predicted_classes_w <- ifelse(fold_predict_w > 0.5, 1, 0)
accuracy_w <- mean(predicted_classes_w == fold_test_w$status)
```

```{r, echo = F}
cat("Accuracy white", accuracy_w, "\n")
```

```{r, include = F}
#accuracy for black
predicted_classes_b <- ifelse(fold_predict_b > 0.5, 1, 0)
accuracy_b <- mean(predicted_classes_b == fold_test_b$status)
```

```{r, echo = F}
cat("Accuracy black", accuracy_b, "\n")
```

```{r, include = F}
#adjusted model
weights <- ifelse(Training$race == "White", 1, 5)

cox_model_weighted <- coxph(Surv(survival_months, status) ~ age + race + 
                             marital_status + t_stage + n_stage + differentiate + 
                             regional_node_examined + reginol_node_positive, 
                           data = Training, weights = weights)

#accuracy of the adjusted model
predicted_classes <- ifelse(predict(cox_model_weighted,type='lp',newdata=Testing) > 0.5, 1, 0)
accuracy <- mean(predicted_classes == Testing$status)
```

```{r, echo = F}
cat("Accuracy", accuracy, "\n")
```



```{r, include = F}
#ROC again
#for race

#all race
fold_test <- Testing
fold_train <- Training

fold_pre <- cox_model_weighted
fold_predict <- predict(fold_pre,type='lp',newdata=fold_test)
roc1<-roc(fold_test$status,fold_predict)

#white
fold_test_w <- Testing |> filter(race == "White")
fold_train_w <- Training |> filter(race == "White")

fold_pre_w <- cox_model_weighted 
fold_predict_w <- predict(fold_pre_w,type='lp',newdata=fold_test_w)
roc2<-roc(fold_test_w$status,fold_predict_w)

#black
fold_test_b <- Testing |> filter(race != "White")
fold_train_b <- Training |> filter(race != "White")

fold_pre_b <- cox_model_weighted 
fold_predict_b <- predict(fold_pre_b,type='lp',newdata=fold_test_b)
roc3<-roc(fold_test_b$status,fold_predict_b)
```

```{r, echo = F}
#plot
plot(roc1, col = "blue", main = "Receiver Operating Characteristic (ROC) Curve", 
     lwd = 2, lty = 1,print.auc=T)

lines(roc2, col = "red", lwd = 2, lty = 2)
lines(roc3, col = "orange", lwd = 2, lty = 3)
```

```{r, include = F}
#accuracy for white
predicted_classes_w <- ifelse(fold_predict_w > 0.5, 1, 0)
accuracy_w <- mean(predicted_classes_w == fold_test_w$status)
```

```{r, echo = F}
cat("Accuracy", accuracy_w, "\n")
```

```{r, include = F}
#accuracy for black
predicted_classes_b <- ifelse(fold_predict_b > 0.5, 1, 0)
accuracy_b <- mean(predicted_classes_b == fold_test_b$status)
```

```{r, echo = F}
cat("Accuracy", accuracy_b, "\n")
```


<!--# Machine Learning -->


<!--## Read Dataset-->
```{r message=FALSE, warning=FALSE, include = F}
breast_cancer<- read.csv("Project_2_data.csv")
attach(breast_cancer)
breast_cancer$Race=as.factor(Race)
breast_cancer$Marital.Status=as.factor(Marital.Status)
breast_cancer$T.Stage=as.factor(T.Stage)
breast_cancer$N.Stage=as.factor(N.Stage)
breast_cancer$X6th.Stage=as.factor(X6th.Stage)
breast_cancer$differentiate=as.factor(differentiate)
breast_cancer$Grade=as.factor(Grade)
breast_cancer$A.Stage=as.factor(A.Stage)
breast_cancer$Estrogen.Status=as.factor(Estrogen.Status)
breast_cancer$Progesterone.Status=as.factor(Progesterone.Status)
breast_cancer$Status=as.factor(Status)
attach(breast_cancer)
```

<!--## Splite data into training and testing-->
```{r message=FALSE, warning=FALSE, include = F}
set.seed(1)
split <- sample.split(breast_cancer, SplitRatio = 0.7)
BC_train <- subset(breast_cancer, split == "TRUE")
BC_test <- subset(breast_cancer, split == "FALSE")

```

## Method
### Decision Tree

The decision tree is an algorithm for solving classification problems, and the model uses a tree structure. Each node of the decision tree follows the if-then-else rule. Specifically, each node of the tree is judged using a certain attribute value, and based on the judgment result, it is decided which branch to enter until the leaf is reached. In addition, decision tree model is not restricted by any strict assumptions. 

1. Obtain the data \( D = \{ (x_n, y_n) \}_{n=1}^N \).
2. Split the data into two regions \( R_1 \) and \( R_2 \), where \( R_1(s, w) = \{ x : x \leq w \} \) and \( R_2(s, w) = \{ x : x \geq w \} \) for any \( x \in \mathbb{R}^d \).
3. Find \( s \) and \( w \) such that the \( RSS(s, w) \) is minimized:
   \[ RSS(s, w) = \sum_{x_i \in R_1} (y_i - \bar{y}_{R_1})^2 + \sum_{x_i \in R_2} (y_i - \bar{y}_{R_2})^2 \]
4. Repeat Step 1-3 to split the regions based on the regions that could lead to the largest decrease in \( RSS \).


## Decision Tree
```{r message=FALSE, warning=FALSE, include = F}
set.seed(2)
# Decision Tree Model
dt_model <- rpart(Status ~ ., data = BC_train, method = "class")

# Plot Decision Tree 
library(rpart)
library(rpart.plot)
```

```{r, echo = F}
rpart.plot(dt_model,type=2)
```

```{r, include = F}
# Examine the complexity plot
printcp(dt_model)
BC_test$pred_base  <- predict(dt_model, BC_test, type = "class")
dt_accuracy <- mean(BC_test$pred_base == BC_test$Status) ## 89.18919%
```
\

The accuracy of decision tree model = 89.19%

## Method
### Random Forest Model

Random forest is a model composed of many independent decision trees, used for classification problems. Random forest uses bootstrap sampling, which is a self-service sampling method, to reduce variance and overfitting problems. Specifically, a data set D containing p predictors is randomly sampled n times with replacement to obtain a new dataset containing m predictors. Grow a decision tree using only m<p predictors. The random forest model then evaluates and averages the classification results of each decision tree and treats it as the result. Random forest is not affected by variable multicollinearity and non-constant variance problems.

1. Obtain the data \( D = \{ (x_n, y_n) \}_{n=1}^N \).
2. Draw a bootstrap sample \( (x_1^{(b)}, y_1^{(b)}), (x_2^{(b)}, y_2^{(b)}), \ldots, (x_n^{(b)}, y_n^{(b)}) \), for \( b = 1,2,\ldots,B \).
3. Grow \( n \) decision tree based on the bootstrap sample with only \( m \) selected variables.
4. Evaluate the prediction result from \( B \) bootstrap samples \( \hat{y}^{(1)}, \hat{y}^{(2)}, \ldots, \hat{y}^{(B)} \), obtain the final result \( \hat{y} = B^{-1} \sum_{b=1}^B \hat{y}^{(b)} \).


## Random Forest Model
```{r, include = F}
# Setting seed
set.seed(120)

## mtry=5
# Build RF Model
classifier_RF_5 = randomForest(x = BC_train[-16],
                             y = BC_train$Status,
                             mtry=5, importance=TRUE,
                             ntree = 500)
  
# Predicting the Test set results
y_pred_5 = predict(classifier_RF_5, newdata = BC_test[-16])
rf_accuracy_5 <- mean(y_pred_5 == BC_test$Status) # Accuracy = 89.50715


## mtry=6
# Build RF Model
classifier_RF_6 = randomForest(x = BC_train[-16],
                             y = BC_train$Status,
                             mtry=6, importance=TRUE,
                             ntree = 500)
  
# Predicting the Test set results
y_pred_6 = predict(classifier_RF_6, newdata = BC_test[-16])
rf_accuracy_6 <- mean(y_pred_6 == BC_test$Status) # Accuracy = 89.26868%

## mtry=7
# Build RF Model
classifier_RF_7 = randomForest(x = BC_train[-16],
                             y = BC_train$Status,
                             mtry=7, importance=TRUE,
                             ntree = 500)
  
# Predicting the Test set results
y_pred_7 = predict(classifier_RF_7, newdata = BC_test[-16])
rf_accuracy_7 <- mean(y_pred_7 == BC_test$Status) # Accuracy = 89.50715

## mtry=8
# Build RF Model
classifier_RF_8 = randomForest(x = BC_train[-16],
                             y = BC_train$Status,
                             mtry=8, importance=TRUE,
                             ntree = 500)
  
# Predicting the Test set results
y_pred_8 = predict(classifier_RF_8, newdata = BC_test[-16])
rf_accuracy_8 <- mean(y_pred_8 == BC_test$Status) # Accuracy = 89.50715%

## mtry=9
# Build RF Model
classifier_RF_9 = randomForest(x = BC_train[-16],
                             y = BC_train$Status,
                             mtry=9, importance=TRUE,
                             ntree = 500)
  
# Predicting the Test set results
y_pred_9 = predict(classifier_RF_9, newdata = BC_test[-16])
rf_accuracy_9 <- mean(y_pred_9 == BC_test$Status) # Accuracy = 89.34817%

## mtry=10
# Build RF Model
classifier_RF_10 = randomForest(x = BC_train[-16],
                             y = BC_train$Status,
                             mtry=10, importance=TRUE,
                             ntree = 500)
  
# Predicting the Test set results
y_pred_10 = predict(classifier_RF_10, newdata = BC_test[-16])
rf_accuracy_10 <- mean(y_pred_10 == BC_test$Status) # Accuracy = 89.50715%

## mtry=11
# Build RF Model
classifier_RF_11 = randomForest(x = BC_train[-16],
                             y = BC_train$Status,
                             mtry=11, importance=TRUE,
                             ntree = 500)
  
# Predicting the Test set results
y_pred_11 = predict(classifier_RF_11, newdata = BC_test[-16])
rf_accuracy_11 <- mean(y_pred_11 == BC_test$Status) # Accuracy = 89.58665% max

## mtry=12
# Build RF Model
classifier_RF_12 = randomForest(x = BC_train[-16],
                             y = BC_train$Status,
                             mtry=12, importance=TRUE,
                             ntree = 500)
  
# Predicting the Test set results
y_pred_12 = predict(classifier_RF_12, newdata = BC_test[-16])
rf_accuracy_12 <- mean(y_pred_12 == BC_test$Status) # Accuracy = 89.42766%

accuracy <- c(rf_accuracy_5,rf_accuracy_6,rf_accuracy_7,rf_accuracy_8,rf_accuracy_9,rf_accuracy_10,
      rf_accuracy_11,rf_accuracy_12)


## Plot Accuracy of different m value
library(ggplot2)
df <- data.frame(x = 5:12, y =c(rf_accuracy_5,rf_accuracy_6,rf_accuracy_7,rf_accuracy_8,
                                rf_accuracy_9,rf_accuracy_10,rf_accuracy_11,rf_accuracy_12))
```

```{r, echo = F}
# Create a line plot using ggplot2
ggplot(df, aes(x, y)) +
  geom_line(color = "#4CAF50", size = 2) +
  geom_point(color = "#FFC107", size = 4) +
  labs(title = "Compare the accuracy of different m", x = "# of predictors", y = "Prediction Accuracy") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 16),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 14))
```

```{r message=FALSE, warning=FALSE, include = F}
# Predicting the Test set results
y_pred_11 = predict(classifier_RF_11, newdata = BC_test[-16])
rf_accuracy_11 <- mean(y_pred_11 == BC_test$Status) # Accuracy = 89.4277%

# Confusion Matrix
confusion_mtx = table(BC_test[,16], y_pred_11)
confusion_mtx
max(confusion_mtx)
```

```{r, echo = F}
# Plotting model
plot(classifier_RF_11)
```

```{r, include = F}
# Importance plot
importance(classifier_RF_11)
```

```{r, echo = F}
# Variable importance plot
varImpPlot(classifier_RF_11)
```
\ 

The accuracy of random forest model = 89.43%


<!--## Access the accuracy of white and black-->
```{r message=FALSE, warning=FALSE, include = F}
## Extract testing data based on different race levels
Test_White <- subset(BC_test, Race == "White")
Test_Black <- subset(BC_test, Race == "Black")

## Decision Tree
Test_White$pred_base  <- predict(dt_model, Test_White, type = "class")
Accuracy_White_DT <- mean(Test_White$pred_base == as.factor(Test_White$Status)) ## White Accuracy = 89.57346%

Test_Black$pred_base  <- predict(dt_model, Test_Black, type = "class")
Accuracy_Black_DT <- mean(Test_Black$pred_base == as.factor(Test_Black$Status)) ## Black Accuracy = 84.00%

## Random Forest
y_pred_white_rf = predict(classifier_RF_11, newdata = Test_White[-16])
Accuracy_White_RF <- mean(y_pred_white_rf == as.factor(Test_White$Status)) ## White Accuracy = 89.95261%

y_pred_black_rf = predict(classifier_RF_11, newdata = Test_Black[-16])
Accuracy_Black_RF <- mean(y_pred_black_rf == as.factor(Test_Black$Status)) ## Black Accuracy = 87.00%
```

* Decision Tree White Accuracy = 89.57%
* Decision Tree Black Accuracy = 84.00%
* Random Forest White Accuracy = 89.86%
* Random Forest Black Accuracy = 86.00%


```{r, echo = F}

## Plot Accuracy of different m value
library(ggplot2)

# Create a data frame with the provided data
data <- data.frame(
  Model = c("logistic_white", "logistic_black", "survival_white", "survival_black", "dt_white", "dt_black", "rf_white", "rf_black"),
  Score = c(0.8805687, 0.8500000, 0.6682464, 0.6256158, 0.8957350, 0.8400000, 0.8995260, 0.8700000)
)

# Create a line chart using ggplot2
ggplot(data, aes(x = Model, y = Score, group = 1)) +
  geom_line(color = "blue") +
  geom_point(color = "red", size = 3) +
  labs(title = "Race Accuracy",
       x = "Different Methods",
       y = "Accuracy") +
  theme_minimal()

```

## Results
### Dataset Description

The dataset “breast cancer” includes 4025 observations and 16 variables. The original dataset is split into a training set (70%) and a testing set (30%). The detailed variable description and summary statistics of numerical and categorical variables are presented in Table1 and Table2.

## Results
### Logistic Regression

“trainOut” removes two outliers 1004 and 2114, shown in Fig 8, and two of three correlated variables n_stage and x6th_stage, shown in Fig 9. “new_model” indicates a refined regression without outliers and correlated variables to meet assumptions. Significant variables include age, race, t stage, differentiate, estrogen status, progesterone status, regional node examined, reginol node positive, and survival months. After the pairwise interaction evaluation process, interactions between survival_months, t_stage, estrogen_status, progesterone_status, and reginol_node_positive are significant and selected into “best_fit”. Following two rounds of model selection, the finalized model, “best_fit2”, is defined as status ~ age + race + t stage + differentiate + estrogen status + progesterone status + reginol node examined + reginol node positive + survival months + race:survival months + t stage:survival months + differentiate:progesterone status + differentiate:survival months + progesterone status:survival months + reginol node positive:survival months. (full model shown below table3). The accuracy of best_fit2 applied to the testing data is 88.00%. Notably, the accuracy for the majority race group White is 88.06%, while for the minority group Black, it stands at 85.00%, shown in table3.   

## Results
### Survival Analysis

After building the full Cox model with all variables that were interested in and several interaction terms, the result of the “cox.zph()” function was displayed in Table 4. The residuals of these covariables against time were shown in Fig 1. It is noticeable that their residuals had certain patterns, which could act as evidence to indicate that these four covariables may violate the assumptions of the Cox model. To keep the GLOBAL p-value (0.87) non-significant, those four variables were excluded, and a new Cox model was established with remaining covariables.
The variable selection process was performed by backward selection, and the final model was constructed as below, and the parameter of the fitted model is shown in Table 5. 
Surv(survival_months, status) ~ age + race + marital_status + t_stage + n_stage + differentiate + regional_node_examined + reginol_node_positive
Finally, the model was adjusted by adding a “weight” parameter to balance the sample size of different races, the prediction accuracy of the original final model and model adjusted by race were calculated for all races (White, Black, and Others). Results were displayed in Table 6 and ROC curves for three classes were included in Fig 2.

## Results
### Decision Tree

The fitted decision tree model is shown in Fig3. The feature selection in the decision tree model is a black box algorithm to obtains the most efficient and accurate model. The decision tree model uses 4 variables for classification, including survival month, age, differential, and marital status. By using the testing dataset, the prediction accuracy of the decision tree is 89.19%, the prediction accuracy for the white race is 89.57%, and for the black race is 84.00%.

## Results
### Random Forest

Random Forest evaluates 500 decision tree models and obtains average results.  After testing the accuracy of 5 -12 variables to grow the tree, the results show that using 11 variables has the highest accuracy, reaching 89.5867% (Fig 4). The prediction accuracy for the white race is 89.9526%, and the accuracy for the black is 87.00%.
In addition,  is also worth noting that the importance of each variable in the random forest tree is obtained by comparing the “Mean Decrease Accuracy”, that is, how much the accuracy of the model will decrease if that variable is removed from the model. According to Fig 5, the variable survival month is the most useful one, followed by reginol node-positive, progesterone status, age, X6th stage, and so on. 


## Conclusion

In the logistic regression, the best_fit2 model suggests that with all else equal, the log odds for t_stage T2 are 0.166 lower than those for t_stage T1, while the log odds for t_stage T3 are 0.214 lower than t_stage T1. Similar interpretations apply to other covariates from Table 1. The impact of selected covariates on status is influenced by the interaction with survival month. The 0.88 accuracy suggests correct predictions for a significant portion (88%) of the dataset. Differences in accuracy between race groups hint at slightly better performance in the White group.
In the survival analysis, Age, T stage, N stage, Regional Node Examined, and Reginol Node Positive had significantly affected the survival of a breast cancer patient. Furthermore, by comparing the predictive accuracy of the final model across race groups, the model exhibited slightly superior predictive performance for White individuals (71.56%) compared to black (67.49%). This observation is further supported by the AUC, where the AUC for black individuals was found to be lower than that for white counterparts. 
After adjusting the model by introducing “weight” parameters associated with the race variable during the model refinement process, analysis of the adjusted model’s accuracy metrics and ROC curves reveal that the augmentation of weights, although leading to an overall decline in the model’s performance, concurrently results in a reduction of the accuracy gap in predicting survival outcomes among individuals of diverse racial backgrounds.
The decision tree model gives a promising prediction accuracy of 89.19% and it includes survival month, age, differential, and marital status. According to the model in Fig 3, if the individual’s survival month is greater than or equal to 48 months, the probability that the individual will be alive is 89%. If the individual’s survival month is less than 48 months and the individual is less than 56 years old and the Tumor size is less than 16, then the probability of being alive is only 1%. The interpretation in other branches is similar.
The prediction accuracy of random forest using 11 variables is the most optimal one, with an accuracy of 89.59%. The random forest does not give a model with a parameter but based on the “Mean Decrease Accuracy”, the model uses the following 11 variables to perform the classification: Survival.Months, Reginol.Node.Positive, Progesterone.Status, Age, X6th.Stage, Estrogen.Status, Tumor.Size, N.Stage, Regional.Node.Examined, differentiate and T.Stage. It is worth noting that the importance of variable survival month is much higher than other variables. 
Generally, four models are used to predict the status of breast cancer patients. The prediction accuracy between different methods is shown in Fig6. The performance of the random forest is the best, with an accuracy of 89.59%.  The prediction accuracy of the White and Black Race by different methods is shown in Fig7. The accuracy between different races by the same method does not differ a lot. Therefore, our model is not limited by race type. 

## Reference

[1]	Loibl, Sibylle et al. “Breast cancer.” Lancet (London, England) vol. 397,10286 (2021): 1750-1769. doi:10.1016/S0140-6736(20)32381-3

[2]	Arnold, Melina et al. “Current and future burden of breast cancer: Global statistics for 2020 and 2040.” Breast (Edinburgh, Scotland) vol. 66 (2022): 15-23. doi:10.1016/j.breast.2022.08.010




